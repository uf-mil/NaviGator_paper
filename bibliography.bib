
@article{AdaptiveControl2018,
  title = {Adaptive Control},
  copyright = {Creative Commons Attribution-ShareAlike License},
  abstract = {Adaptive control is the control method used by a controller which must adapt to a controlled system with parameters which vary, or are initially uncertain.  For example, as an aircraft flies, its mass will slowly decrease as a result of fuel consumption;  a control law is needed that adapts itself to such changing conditions.  Adaptive control is different from robust control in that it does not need a priori information about the bounds on these uncertain or time-varying parameters; robust control guarantees that if the changes are within given bounds the control law need not be changed, while adaptive control is concerned with control law changing itself.},
  language = {en},
  journal = {Wikipedia},
  month = nov,
  year = {2018},
  file = {/home/daniel/Zotero/storage/USMPRC7R/index.html},
  note = {Page Version ID: 868735676}
}

@article{devarakondaAvoidingCommunicationPrimal2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.04003},
  primaryClass = {cs},
  title = {Avoiding Communication in Primal and Dual Block Coordinate Descent Methods},
  abstract = {Primal and dual block coordinate descent methods are iterative methods for solving regularized and unregularized optimization problems. Distributed-memory parallel implementations of these methods have become popular in analyzing large machine learning datasets. However, existing implementations communicate at every iteration which, on modern data center and supercomputing architectures, often dominates the cost of floating-point computation. Recent results on communication-avoiding Krylov subspace methods suggest that large speedups are possible by reorganizing iterative algorithms to avoid communication. We show how applying similar algorithmic transformations can lead to primal and dual block coordinate descent methods that only communicate every s iterations\textendash{}where s is a tuning parameter\textendash{}instead of every iteration for the regularized least-squares problem. We show that the communication-avoiding variants reduce the number of synchronizations by a factor of s on distributed-memory parallel machines without altering the convergence rate and attains strong scaling speedups of up to 6.1\texttimes{} on a Cray XC30 supercomputer.},
  language = {en},
  journal = {arXiv:1612.04003 [cs]},
  author = {Devarakonda, Aditya and Fountoulakis, Kimon and Demmel, James and Mahoney, Michael W.},
  month = dec,
  year = {2016},
  keywords = {68W10; 65F10,Computer Science - Distributed; Parallel; and Cluster Computing,G.1.0,G.1.3,G.1.6},
  file = {/home/daniel/Zotero/storage/GUNTSKH9/Devarakonda et al. - 2016 - Avoiding communication in primal and dual block co.pdf}
}

@techreport{lavalleRapidlyExploringRandomTrees1998,
  title = {Rapidly-{{Exploring Random Trees}}: {{A New Tool}} for {{Path Planning}}},
  shorttitle = {Rapidly-{{Exploring Random Trees}}},
  abstract = {We introduce the concept of a Rapidly-exploring Random Tree (RRT) as a randomized data structure that is designed for a broad class of path planning problems. While they share many of the beneficial properties of existing randomized planning techniques, RRTs are specifically designed to handle nonholonomic constraints (including dynamics) and high degrees of freedom. An RRT is iteratively expanded by applying control inputs that drive the system slightly toward randomly-selected points, as opposed to requiring point-to-point convergence, as in the probabilistic roadmap approach. Several desirable properties and a basic implementation of RRTs are discussed. To date, we have successfully applied RRTs to holonomic, nonholonomic, and kinodynamic planning problems of up to twelve degrees of freedom.},
  author = {Lavalle, Steven M.},
  year = {1998},
  file = {/home/daniel/Zotero/storage/SGYR593I/Lavalle - 1998 - Rapidly-Exploring Random Trees A New Tool for Pat.pdf;/home/daniel/Zotero/storage/7D68MKMZ/summary.html}
}

@misc{COCOCommonObjects,
  title = {{{COCO}} - {{Common Objects}} in {{Context}}},
  howpublished = {http://cocodataset.org/\#home},
  file = {/home/daniel/Zotero/storage/39XS6K3S/cocodataset.org.html}
}

@article{torreyTransferLearning2010,
  title = {Transfer {{Learning}}},
  copyright = {Access limited to members},
  doi = {10.4018/978-1-60566-766-9.ch011},
  abstract = {Transfer Learning: 10.4018/978-1-60566-766-9.ch011: Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned. While most},
  language = {en},
  journal = {Handbook of Research on Machine Learning Applications and Trends: Algorithms, Methods, and Techniques},
  author = {Torrey, Lisa and Shavlik, Jude},
  year = {2010},
  pages = {242-264},
  file = {/home/daniel/Zotero/storage/5NCFH4UP/36988.html}
}

@misc{ComputationalApproachEdge,
  title = {A {{Computational Approach}} to {{Edge Detection}} - {{IEEE Journals}} \& {{Magazine}}},
  howpublished = {https://ieeexplore.ieee.org/document/4767851},
  file = {/home/daniel/Zotero/storage/TNCIJQPX/4767851.html}
}

@misc{ModelZooDeep,
  title = {Model {{Zoo}} - {{Deep}} Learning Code and Pretrained Models for Transfer Learning, Educational Purposes, and More},
  howpublished = {https://www.modelzoo.co/},
  file = {/home/daniel/Zotero/storage/LBI23RL3/www.modelzoo.co.html}
}


